{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Prerequisit\n",
    "1) MySql jar is added in classpath of Spark\n",
    "> You can do it by having a symbolic link in SPARK_HOME/jars to MySql jar\n",
    ">> `~$` ln -s /usr/share/java/mysql-connector-java-5.1.45.jar /home/talentum/spark/jars/mysql-connector-java.jar\n",
    "Ref - https://www.cyberciti.biz/faq/creating-soft-link-or-symbolic-link/\n",
    "\n",
    "2) cd to ~/test-jupyter/test/ on your apache sandbox\n",
    "\n",
    "3) test`$` cp salaries.txt /tmp\n",
    "\n",
    "4) test`$` mysql -u bigdata -p\n",
    "password Bigdata@123\n",
    "\n",
    "5) mysql>CREATE DATABASE test;\n",
    "\n",
    "6) Mysql>use test;\n",
    "\n",
    "7) Mysql>drop table if exists salaries;\n",
    "\n",
    "8) Mysql>create table salaries (\n",
    "gender varchar(1),\n",
    "age int,\n",
    "salary double,\n",
    "zipcode int);\n",
    "\n",
    "9) Mysql>load data local infile '/tmp/salaries.txt' into table salaries fields terminated by ',';\n",
    "\n",
    "10) Mysql>alter table salaries add column `id` int(10) unsigned primary KEY AUTO_INCREMENT;\n",
    "\n",
    "11) Quit MySql\n",
    "> Mysql>quit;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialization\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/talentum/spark\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.6\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "# NOTE: Whichever package you want mention here.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0 pyspark-shell' \n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.3 pyspark-shell'\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrypoint 2.x\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark SQL basic example\").getOrCreate()\n",
    "\n",
    "# On yarn:\n",
    "# spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().master(\"yarn\").getOrCreate()\n",
    "# specify .master(\"yarn\")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyspark working with MySql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"jdbc:mysql://127.0.0.1:3306/test?useSSL=false&allowPublicKeyRetrieval=true\"\n",
    "driver = \"com.mysql.jdbc.Driver\"\n",
    "user = \"bigdata\"\n",
    "password = \"Bigdata@123\"\n",
    "\n",
    "# https://youtu.be/ray3YvnIohM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =  spark.read\\\n",
    "    .format(\"jdbc\")\\\n",
    "    .option(\"driver\", driver)\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"user\", user)\\\n",
    "    .option(\"password\", password)\\\n",
    "    .option(\"dbtable\", \"salaries\")\\\n",
    "    .load()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+-------+-------+---+\n",
      "|gender|age| salary|zipcode| id|\n",
      "+------+---+-------+-------+---+\n",
      "|     F| 66|41000.0|  95103|  1|\n",
      "|     M| 40|76000.0|  95102|  2|\n",
      "|     F| 58|95000.0|  95103|  3|\n",
      "|     F| 68|60000.0|  95105|  4|\n",
      "|     M| 85|14000.0|  95102|  5|\n",
      "+------+---+-------+-------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df =  spark.read\\\n",
    "    .format(\"jdbc\")\\\n",
    "    .option(\"driver\", driver)\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"user\", user)\\\n",
    "    .option(\"password\", password)\\\n",
    "    .option(\"dbtable\", \"salaries\")\\\n",
    "    .load()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading From Database in Parallel\n",
    "\n",
    "When we are reading large table, we would like to read that in parallel. This will dramatically improve read performance. We can pass “numPartitions” option to spark read function which will decide parallelism in reading data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =  spark.read\\\n",
    "    .format(\"jdbc\")\\\n",
    "    .option(\"driver\", driver)\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"user\", user)\\\n",
    "    .option(\"password\", password)\\\n",
    "    .option(\"dbtable\", \"salaries\")\\\n",
    "    .option(\"numPartitions\", 10)\\\n",
    "    .load()\n",
    " \n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, it will still show as 1 partition only. This is because we do not have enough data to create 10 different partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
