{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aec49ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialization\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/talentum/spark\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.6\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "# NOTE: Whichever package you want mention here.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0 pyspark-shell' \n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.3 pyspark-shell'\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42329f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrypoint 2.x\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "# On yarn:\n",
    "# spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().master(\"yarn\").getOrCreate()\n",
    "# specify .master(\"yarn\")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "638de57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('of', 493)\n",
      "('United', 85)\n",
      "('in', 137)\n",
      "('Order', 1)\n",
      "('form', 1)\n",
      "('more', 10)\n",
      "('perfect', 1)\n",
      "('Union,', 4)\n",
      "('establish', 4)\n",
      "('Tranquility,', 1)\n",
      "\n",
      "the has 662 counts\n",
      "of has 493 counts\n",
      "shall has 293 counts\n",
      "and has 256 counts\n",
      "to has 183 counts\n",
      "be has 178 counts\n",
      "or has 157 counts\n",
      "in has 137 counts\n",
      "by has 100 counts\n",
      "a has 94 counts\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#file_path = \"file://<pwd>/Dataset/Complete_Shakespeare.txt\"\n",
    "file_path = \"file:////home/talentum/shared/4_AdvancedRddActions/Dataset/constitution.txt\"\n",
    "\n",
    "# Create a baseRDD from the file path\n",
    "baseRDD = sc.textFile(file_path)\n",
    "\n",
    "# Split the lines of baseRDD into words\n",
    "splitRDD = baseRDD.flatMap(lambda x: x.split(' '))\n",
    "\n",
    "# Removing blank space\n",
    "splitRDD_no_stop = splitRDD.filter(lambda x: x != '')\n",
    "\n",
    "# Create a tuple of the word and 1 \n",
    "splitRDD_no_stop_words = splitRDD_no_stop.map(lambda w: (w, 1))\n",
    "\n",
    "# Count of the number of occurences of each word\n",
    "resultRDD = splitRDD_no_stop_words.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Display the first 10 words and their frequencies\n",
    "for word in resultRDD.take(10):\n",
    "    print(word)\n",
    "\n",
    "print()\n",
    "# Swap the keys and values \n",
    "resultRDD_swap = resultRDD.map(lambda x: (x[1], x[0]))\n",
    "\n",
    "# Sort the keys in descending order\n",
    "resultRDD_swap_sort = resultRDD_swap.sortByKey(ascending=False)\n",
    "\n",
    "# Show the top 10 most frequent words and their frequencies\n",
    "for word in resultRDD_swap_sort.take(10):\n",
    "    print(\"{} has {} counts\". format(word[1], word[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25c5c886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(812, ''),\n",
       " (662, 'the'),\n",
       " (493, 'of'),\n",
       " (293, 'shall'),\n",
       " (256, 'and'),\n",
       " (183, 'to'),\n",
       " (178, 'be'),\n",
       " (157, 'or'),\n",
       " (137, 'in'),\n",
       " (100, 'by')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"file:////home/talentum/shared/4_AdvancedRddActions/Dataset/constitution.txt\"\n",
    "\n",
    "# Create a baseRDD from the file path\n",
    "baseRDD = sc.textFile(file_path)\n",
    "\n",
    "# Convert line to words\n",
    "rdd_words = baseRDD.flatMap(lambda line : line.split(' '))\n",
    "\n",
    "# Converting words to tuple\n",
    "rdd_tup = rdd_words.map(lambda word: (word , 1))\n",
    "\n",
    "# Counting frequency of each word\n",
    "rdd_reduce = rdd_tup.reduceByKey(lambda x, y : x + y)\n",
    "\n",
    "# Swaping the values in tuple\n",
    "rdd_swap = rdd_reduce.map(lambda tup : (tup[1], tup[0]))\n",
    "\n",
    "# Sorting the values\n",
    "rdd_sort = rdd_swap.sortByKey(ascending = False).take(10)\n",
    "rdd_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bb376ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(812, ''),\n",
       " (662, 'the'),\n",
       " (493, 'of'),\n",
       " (293, 'shall'),\n",
       " (256, 'and'),\n",
       " (183, 'to'),\n",
       " (178, 'be'),\n",
       " (157, 'or'),\n",
       " (137, 'in'),\n",
       " (100, 'by')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#same code in single line (Chaining)\n",
    "\n",
    "file_path = \"file:////home/talentum/shared/4_AdvancedRddActions/Dataset/constitution.txt\"\n",
    "\n",
    "\n",
    "sc.textFile(file_path) \\\n",
    ".flatMap(lambda line : line.split(' ')) \\\n",
    ".map(lambda word: (word , 1)) \\\n",
    ".reduceByKey(lambda x, y : x + y) \\\n",
    ".map(lambda tup : (tup[1], tup[0])) \\\n",
    ".sortByKey(ascending = False).take(10) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
