{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3726fc33",
   "metadata": {},
   "source": [
    "# Map and Collect\n",
    "\n",
    "- The main method by which you can manipulate data in PySpark is using `map()`. The `map()` transformation takes in a function and applies it to each element in the RDD. It can be used to do any number of things, from fetching the website associated with each URL in our collection to just squaring the numbers. In this simple exercise, you'll use `map()` transformation to cube each number of the `numbRDD` RDD that you created earlier. Next, you'll return all the elements to a variable and finally print the output.\n",
    "\n",
    "- Remember, you already have a `SparkContext` `sc`, and `numbRDD` available in your workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7fd167",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "- Create `map()` transformation that cubes all of the numbers in `numbRDD`.\n",
    "- Collect the results in a `numbers_all` variable.\n",
    "- Print the output from `numbers_all` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0de212f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialization\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/talentum/spark\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.6\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "# NOTE: Whichever package you want mention here.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0 pyspark-shell' \n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.3 pyspark-shell'\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc0b6d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrypoint 2.x\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "# On yarn:\n",
    "# spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().master(\"yarn\").getOrCreate()\n",
    "# specify .master(\"yarn\")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94ca69ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of numbers: 5\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "numbRDD = sc.parallelize(numbers)\n",
    "\n",
    "# Create filter() transformation to cube numbers\n",
    "evenRDD = numbRDD.filter(lambda x: x % 2 == 0)\n",
    "\n",
    "# Print the total no. of sentences\n",
    "print(\"Total number of numbers:\", evenRDD.count())\n",
    "\n",
    "# Collect the results\n",
    "numbers_all = evenRDD.collect()\n",
    "\n",
    "# Print the numbers from numbers_all\n",
    "for numb in numbers_all:\n",
    "    print(numb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8fbe3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences: 104\n",
      "['# Apache Spark', '', 'Spark is a fast and general cluster computing system for Big Data. It provides', 'high-level APIs in Scala, Java, Python, and R, and an optimized engine that']\n"
     ]
    }
   ],
   "source": [
    "textRDD = sc.textFile(\"file:///home/talentum/spark/README.md\")\n",
    "\n",
    "\n",
    "# Print the total no. of sentences\n",
    "print(\"Total number of sentences:\", textRDD.count())\n",
    "\n",
    "# Collect the results\n",
    "text_all = textRDD.collect()\n",
    "\n",
    "# Print the starting 4 lines\n",
    "print(text_all[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ef8a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use api saveasTextFile"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
